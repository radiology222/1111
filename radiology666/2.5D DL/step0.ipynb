{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab6750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████▋              | 66/80 [01:47<00:24,  1.76s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from skimage.measure import label, regionprops\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import psutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# 初始化日志\n",
    "logging.basicConfig(\n",
    "    filename=f'segmentation_log_{datetime.now().strftime(\"%Y%m%d_H%M%S\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# 常量定义\n",
    "MIN_CLUSTERS = 3\n",
    "MAX_CLUSTERS = 100\n",
    "CLUSTER_SEARCH_RANGE = range(5, 51, 5)  # 用于寻找最佳聚类数的搜索范围\n",
    "MIN_SAMPLE_POINTS = 50  # 用于聚类数搜索的最小采样点数\n",
    "MAX_FEATURES = 250000  # 最大特征点数\n",
    "BATCH_SIZE = 64  # 分块处理大小\n",
    "\n",
    "def check_memory_safety(required_mb):\n",
    "    \"\"\"检查内存是否足够\"\"\"\n",
    "    free_mem = psutil.virtual_memory().available / (1024 ** 2)\n",
    "    if free_mem < required_mb * 1.2:\n",
    "        raise MemoryError(f\"需要{required_mb}MB内存，当前可用仅{free_mem:.1f}MB\")\n",
    "\n",
    "def get_voxel_size(img):\n",
    "    \"\"\"获取体素物理尺寸(mm)\"\"\"\n",
    "    return img.header.get_zooms()\n",
    "\n",
    "def normalize_data(data, roi_mask):\n",
    "    \"\"\"基于ROI区域的智能归一化 (内存优化版)\"\"\"\n",
    "    if np.count_nonzero(roi_mask) == 0:\n",
    "        return data\n",
    "    \n",
    "    # 分块计算统计量\n",
    "    sum_total = 0.0\n",
    "    sum_sq = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for z in range(data.shape[2]):\n",
    "        slice_mask = roi_mask[:, :, z]\n",
    "        if np.any(slice_mask):\n",
    "            slice_data = data[:, :, z][slice_mask]\n",
    "            sum_total += np.sum(slice_data, dtype=np.float64)\n",
    "            sum_sq += np.sum(slice_data**2, dtype=np.float64)\n",
    "            count += np.count_nonzero(slice_mask)\n",
    "    \n",
    "    if count == 0:\n",
    "        return data\n",
    "    \n",
    "    mean_val = sum_total / count\n",
    "    std_val = np.sqrt((sum_sq / count) - (mean_val ** 2))\n",
    "    \n",
    "    # 分块归一化\n",
    "    for z in range(data.shape[2]):\n",
    "        data[:, :, z] = (data[:, :, z] - mean_val) / (std_val + 1e-8)\n",
    "    \n",
    "    np.nan_to_num(data, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return data\n",
    "\n",
    "def calculate_silhouette(features, k):\n",
    "    \"\"\"计算单个k值的轮廓系数\"\"\"\n",
    "    if len(features) < k:\n",
    "        return -1\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=k,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_init=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    return silhouette_score(features, labels) if k > 1 else 0\n",
    "\n",
    "def find_optimal_clusters(features):\n",
    "    \"\"\"\n",
    "    使用轮廓系数寻找最佳聚类数\n",
    "    返回: (最佳聚类数, 轮廓系数)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 限制特征点数量以提高速度\n",
    "        sample_size = min(MAX_FEATURES, len(features))\n",
    "        if sample_size < MIN_SAMPLE_POINTS:\n",
    "            return min(MAX_CLUSTERS, max(MIN_CLUSTERS, sample_size//10)), 0\n",
    "        \n",
    "        sample_idx = np.random.choice(len(features), sample_size, replace=False)\n",
    "        sample_features = features[sample_idx]\n",
    "        \n",
    "        # 并行计算轮廓系数\n",
    "        silhouettes = Parallel(n_jobs=-1)(\n",
    "            delayed(calculate_silhouette)(sample_features, k) \n",
    "            for k in CLUSTER_SEARCH_RANGE\n",
    "        )\n",
    "        \n",
    "        optimal_k = CLUSTER_SEARCH_RANGE[np.argmax(silhouettes)]\n",
    "        return optimal_k, max(silhouettes)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"寻找最佳聚类数失败: {str(e)}\")\n",
    "        return MIN_CLUSTERS, 0\n",
    "\n",
    "def memory_safe_clustering(points, intensities, optimal_k):\n",
    "    \"\"\"内存安全的聚类实现 (优化版)\"\"\"\n",
    "    try:\n",
    "        # 使用32位浮点数\n",
    "        features = np.hstack([\n",
    "            points.astype(np.float32), \n",
    "            intensities.astype(np.float32)\n",
    "        ])\n",
    "        \n",
    "        # 分批次训练\n",
    "        kmeans = MiniBatchKMeans(\n",
    "            n_clusters=optimal_k,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_iter=100,\n",
    "            n_init=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        kmeans.fit(features)\n",
    "        return kmeans.labels_\n",
    "    except MemoryError:\n",
    "        logging.warning(\"内存不足，使用简化聚类\")\n",
    "        return np.zeros(len(points), dtype=int)\n",
    "\n",
    "def postprocess_segmentation(segments, min_voxels=20):\n",
    "    \"\"\"优化后的后处理\"\"\"\n",
    "    # 使用高斯滤波（更快且保留边缘）\n",
    "    segments = gaussian_filter(segments.astype(np.float32), sigma=0.5).astype(np.int16)\n",
    "    \n",
    "    # 移除小区域\n",
    "    labeled = label(segments > 0)\n",
    "    regions = regionprops(labeled)\n",
    "    for reg in regions:\n",
    "        if reg.area < min_voxels:\n",
    "            segments[labeled == reg.label] = 0\n",
    "    return segments\n",
    "\n",
    "def process_single_image(args):\n",
    "    \"\"\"处理单个图像的核心函数（内存优化版）\"\"\"\n",
    "    image_path, mask_path, output_dir, min_lesion_size_mm3 = args\n",
    "    try:\n",
    "        # 1. 验证输入文件\n",
    "        if not all(os.path.exists(p) for p in [image_path, mask_path]):\n",
    "            raise FileNotFoundError(\"输入文件不存在\")\n",
    "            \n",
    "        # 2. 加载数据（使用float32节省内存）\n",
    "        img = nib.load(image_path)\n",
    "        data = img.get_fdata().astype(np.float32)\n",
    "        mask = nib.load(mask_path).get_fdata().astype(bool)\n",
    "        roi = mask > 0\n",
    "        \n",
    "        # 内存检查\n",
    "        voxel_count = np.prod(data.shape)\n",
    "        required_mb = voxel_count * 4 / (1024 ** 2)  # float32占4字节\n",
    "        check_memory_safety(required_mb)\n",
    "        \n",
    "        if not np.any(roi):\n",
    "            logging.info(f\"{os.path.basename(image_path)}: 无有效ROI\")\n",
    "            return None, 0, image_path\n",
    "\n",
    "        # 3. 预处理\n",
    "        voxel_size = get_voxel_size(img)\n",
    "        voxel_volume = np.prod(voxel_size)\n",
    "        data = normalize_data(data, roi)\n",
    "        points = np.argwhere(roi).astype(np.float32)\n",
    "        \n",
    "        # 4. 特征工程（限制采样数量）\n",
    "        sample_step = max(1, len(points)//MIN_SAMPLE_POINTS)\n",
    "        sample_points = points[::sample_step]\n",
    "        sample_intensities = data[tuple(sample_points.T.astype(int))].reshape(-1, 1)\n",
    "        features = np.hstack([sample_points, sample_intensities])\n",
    "        \n",
    "        # 5. 确定最佳聚类数\n",
    "        optimal_k, sil_score = find_optimal_clusters(features)\n",
    "        logging.info(f\"{os.path.basename(image_path)}: 使用{optimal_k}个聚类 (轮廓系数={sil_score:.2f})\")\n",
    "        \n",
    "        # 6. 全量数据聚类\n",
    "        intensities = data[roi].reshape(-1, 1).astype(np.float32)\n",
    "        labels = memory_safe_clustering(points, intensities, optimal_k)\n",
    "        \n",
    "        segments = np.zeros(data.shape, dtype=np.int16)\n",
    "        segments[tuple(points.T.astype(int))] = labels + 1\n",
    "        \n",
    "        # 7. 后处理\n",
    "        min_voxels = max(1, int(min_lesion_size_mm3 / voxel_volume))\n",
    "        segments = postprocess_segmentation(segments, min_voxels)\n",
    "        \n",
    "        # 8. 保存结果\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_path = os.path.join(output_dir, os.path.basename(image_path))\n",
    "            nib.save(nib.Nifti1Image(segments, img.affine), output_path)\n",
    "            \n",
    "        return segments, optimal_k, image_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"处理失败 {os.path.basename(image_path)}: {str(e)}\", exc_info=True)\n",
    "        return None, 0, image_path\n",
    "\n",
    "def batch_process(images_dir, masks_dir, output_dir, min_lesion_size_mm3=50, n_jobs=4):\n",
    "    \"\"\"并行批量处理\"\"\"\n",
    "    # 验证输入目录\n",
    "    if not all(os.path.isdir(d) for d in [images_dir, masks_dir]):\n",
    "        raise ValueError(\"输入目录无效\")\n",
    "    \n",
    "    # 获取文件列表\n",
    "    image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.nii.gz')])\n",
    "    mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith('.nii.gz')])\n",
    "    \n",
    "    if len(image_files) != len(mask_files):\n",
    "        raise ValueError(\"图像与掩膜数量不匹配\")\n",
    "    \n",
    "    # 准备参数\n",
    "    args_list = [(os.path.join(images_dir, img), \n",
    "                 os.path.join(masks_dir, mask),\n",
    "                 output_dir,\n",
    "                 min_lesion_size_mm3) for img, mask in zip(image_files, mask_files)]\n",
    "    \n",
    "    # 内存检查\n",
    "    if psutil.virtual_memory().percent > 90:\n",
    "        logging.warning(\"内存使用过高，建议减少并行任务数\")\n",
    "        n_jobs = max(1, n_jobs//2)\n",
    "    \n",
    "    # 并行处理\n",
    "    logging.info(f\"开始处理 {len(image_files)} 个样本 (并行数={n_jobs})\")\n",
    "    results = Parallel(n_jobs=n_jobs, max_nbytes=None)(\n",
    "        delayed(process_single_image)(args) for args in tqdm(args_list))\n",
    "    \n",
    "    # 收集统计信息\n",
    "    stats = []\n",
    "    for seg, k, img_path in results:\n",
    "        if seg is not None:\n",
    "            unique, counts = np.unique(seg[seg > 0], return_counts=True)\n",
    "            stats.append({\n",
    "                'sample': os.path.basename(img_path),\n",
    "                'optimal_clusters': k,\n",
    "                'total_voxels': np.sum(seg > 0),\n",
    "                'min_region_size': np.min(counts) if len(counts) > 0 else 0,\n",
    "                'median_region_size': np.median(counts) if len(counts) > 0 else 0,\n",
    "                'max_region_size': np.max(counts) if len(counts) > 0 else 0\n",
    "            })\n",
    "    \n",
    "    # 保存统计结果\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    stats_path = os.path.join(output_dir, \"segmentation_stats.csv\")\n",
    "    stats_df.to_csv(stats_path, index=False)\n",
    "    \n",
    "    # 打印摘要\n",
    "    logging.info(\"\\n===== 处理完成 =====\")\n",
    "    logging.info(f\"成功处理: {len(stats)}/{len(image_files)}\")\n",
    "    logging.info(f\"平均聚类数: {stats_df['optimal_clusters'].mean():.1f}\")\n",
    "    logging.info(f\"区域大小分布 (voxels):\")\n",
    "    logging.info(f\"- 最小: {stats_df['min_region_size'].min()}\")\n",
    "    logging.info(f\"- 中位数: {stats_df['median_region_size'].median()}\")\n",
    "    logging.info(f\"- 最大: {stats_df['max_region_size'].max()}\")\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置参数\n",
    "    IMAGES_DIR = r'E:\\00000wwc\\000houai\\isurvival'\n",
    "    MASKS_DIR = r'E:\\00000wwc\\000houai\\msurvival'\n",
    "    OUTPUT_DIR = r'E:\\00000wwc\\000houai\\habitat_optimized_fina2'\n",
    "    MIN_LESION_SIZE_MM3 = 50  # 最小病灶体积(mm³)\n",
    "    N_JOBS = 6  # 并行任务数\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # 运行处理\n",
    "    try:\n",
    "        stats = batch_process(\n",
    "            images_dir=IMAGES_DIR,\n",
    "            masks_dir=MASKS_DIR,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            min_lesion_size_mm3=MIN_LESION_SIZE_MM3,\n",
    "            n_jobs=N_JOBS\n",
    "        )\n",
    "        print(\"处理完成！结果保存在:\", OUTPUT_DIR)\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"主程序失败: {str(e)}\", exc_info=True)\n",
    "        print(f\"运行失败: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
